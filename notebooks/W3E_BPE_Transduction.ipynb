{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JZxptjxEyQl"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gsarti/ik-nlp-tutorials/blob/main/notebooks/W3E_BPE_Transduction.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUTq_HLDEyQn"
      },
      "outputs": [],
      "source": [
        "# Run in Colab to install local packages\n",
        "!pip install spacy transformers datasets\n",
        "!pip install sentencepiece datasets simplet5 tokenizers\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-5EKTaVEyQn"
      },
      "source": [
        "# BPE Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8zWhVZYEyQo"
      },
      "source": [
        "*This exercise follows the explanation of using BPE tokenization as explained on Huggingface [Build a Tokenizer from Scratch](https://huggingface.co/docs/tokenizers/python/latest/quicktour.html#build-a-tokenizer-from-scratch). Adapted from a notebook by Wietse de Vries*\n",
        "\n",
        "The [Tokenizers](https://huggingface.co/docs/tokenizers/python/latest/quicktour.html) library by Huggingface provides implementations of today’s most used tokenizers (especially subword-based ones) that is both easy to use and blazing fast (Rust-compiled code!).\n",
        "\n",
        "You will start by exploring the impact of different vocabulary sizes on a subword tokenizer using the Tokenizers library, and how these can be imported and used with spaCy. Finally, you will be asked to train a small transformer model to perform transduction from feminine to masculine words.\n",
        "\n",
        "Exercise 1 is mandatory and will be part of your graded midterm portfolio. Exercise 2 is optional, but we highly recommend you to complete it, especially if you're interested in the \"Modern Neural Networks meet Linguistic Theory\" final project.\n",
        "\n",
        "## Exercise 1: Byte Pair Encoding with Huggingface Tokenizers\n",
        "\n",
        "In the following exercise, we will use a byte-pair encoding (BPE) tokenizer (see Jurafsky & Martin Sec. 2.4.3 and [Sennich et al, 2015](https://aclanthology.org/P16-1162/) to create a vocabulary of frequent words and subwords, allowing us to handle less frequent words.\n",
        "\n",
        "### Setup\n",
        "\n",
        "The following code loads a BPE tokenizer and trainer, tells the system to use whitespace as a separator and defines `[UNK]` as a special token intended to handle unknown words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPEIIdLCEyQo"
      },
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "trainer = BpeTrainer(special_tokens=[\"[UNK]\"], vocab_size=20000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkexxVzOEyQo"
      },
      "source": [
        "### Corpus\n",
        "\n",
        "The tokenizer creates a dictionary by concatenating characters and substrings into longer strings (possibly full words) based on frequency. So we need a corpus to learn what the most frequent words and substrings are.\n",
        "\n",
        "[Wikitext-103](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) is a dump of the (English) Wikipedia. You can use the `train_from_iterator` method to train from the data in memory, which can be done using the `wikitext` corpus in the [Huggingface Datasets library](https://huggingface.co/datasets/wikitext).\n",
        "\n",
        "### Run the trainer\n",
        "\n",
        "The command below trains the tokenizer on the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11OzuR-AEyQo"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "dataset = datasets.load_dataset(\n",
        "    \"wikitext\", \"wikitext-103-raw-v1\", split=\"train+test+validation\"\n",
        ")\n",
        "\n",
        "# Build a generator to iterate over the dataset\n",
        "def batch_iterator(batch_size=1000):\n",
        "    for i in range(0, len(dataset), batch_size):\n",
        "        yield dataset[i : i + batch_size][\"text\"]\n",
        "\n",
        "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer, length=len(dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSv-PMT4EyQp"
      },
      "source": [
        "### Test the tokenizer\n",
        "\n",
        "Now that we have created a vocabulary, we can use it to tokenize a string into words and subtokens (for infrequent words).\n",
        "\n",
        "The example shows that most of the words are included in the vocabulary created by training on Wikipedia text, but that the acronym *UG*, the name *Hanze*, and the word *Applied*, *jointly* and *initiating* are segmented into subword strings. This suggests that these words were not seen during training, or very infrequently. (*UG* occurs 5 times in the training data and *Applied* over 200 times,  also note that the encoding is case-sensitive.).\n",
        "\n",
        "Try a few other examples to get a feeling for the lexical coverage of the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqtyKkLeEyQp"
      },
      "outputs": [],
      "source": [
        "def show_tokens(text):\n",
        "    output = tokenizer.encode(text)\n",
        "    print(f\"Tokens: {output.tokens}\")\n",
        "    number_of_words = len(tokenizer.pre_tokenizer.pre_tokenize_str(text))\n",
        "    number_of_segments = len(output.tokens)\n",
        "    print(f\"{number_of_words} words and {number_of_segments} segments\")\n",
        "\n",
        "example = \"The UG and the Hanze University of Applied Sciences are jointly initiating a pilot rapid testing centre, which will start on 18 January.\"\n",
        "show_tokens(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRYX4kZ-EyQp"
      },
      "source": [
        "### Your Turn: Experiment with Vocabulary Size\n",
        "\n",
        "The training data contains 103 M tokens and has a vocabulary size of 267,000 unique types. The default setting for the trainer is to create a dictionary of max 30,000 words. This means that a fair amount of compression takes place. Even more compression can be achieved by setting the vocab_size to a smaller value.\n",
        "\n",
        "1. Choose an example text consisting of at least 100 words. You may want to ensure that it contains some rare words or tokens.\n",
        "\n",
        "2. Experiment with various settings for vocab_size.\n",
        "\n",
        "3. Count the number of words in the example, and the number of segments created by the BPE-tokenizer. Note that if the number segments goes up, more words are segmented into subwords.\n",
        "\n",
        "4. What is the vocabulary size where the number of segments is approx. 150% of the number of words?\n",
        "\n",
        "5. For this setting, what was the longest word in your example text that was not segmented?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zh3t6kS3EyQp"
      },
      "outputs": [],
      "source": [
        "# TODO: Try with various vocab_sizes\n",
        "# Important: You will need to redefine the tokenizer for every new vocab size,\n",
        "# otherwise you might incur in an \"PanicError: no entry found for key\" exception\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "trainer = BpeTrainer(special_tokens=[\"[UNK]\"],vocab_size=30000)\n",
        "\n",
        "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer, length=len(dataset))\n",
        "\n",
        "test_text = \"Enter some English text containing at least 100 words\"\n",
        "\n",
        "show_tokens(test_text)\n",
        "\n",
        "# Answer question 5 by going over the output, or write a\n",
        "# few lines of code to provide the answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTYUxVLeEyQp"
      },
      "source": [
        "### Loading the BPE Tokenizer into spaCy\n",
        "\n",
        "Now that you experimented with the creation of many tokenizers using Huggingface Tokenizers, you might want to move them to a more familiar environment. The following class lets you load a Huggingface Tokenizer into spaCy: the `get_words_spaces` function is used to preserve the whitespaces before tokens that are not word pieces.\n",
        "\n",
        "### Your Turn: Fill in the missing code\n",
        "\n",
        "Your task is to complete the `__call__` method of the `BPETokenizer` class to go from text to spaCy `Docs`, and finally to print the tokenized text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBuDa8QEEyQq"
      },
      "outputs": [],
      "source": [
        "from spacy.tokens import Doc\n",
        "from spacy.vocab import Vocab\n",
        "import spacy\n",
        "\n",
        "class BPETokenizer:\n",
        "    def __init__(self, tokenizer, vocab):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def get_words_spaces(self, tokens):\n",
        "        words = []\n",
        "        spaces = []\n",
        "        for i, (text, (_, end)) in enumerate(\n",
        "            zip(tokens.tokens, tokens.offsets)\n",
        "        ):\n",
        "            words.append(text)\n",
        "            if i < len(tokens.tokens) - 1:\n",
        "                # If next start != current end we assume a\n",
        "                # space in between\n",
        "                next_start, _ = tokens.offsets[i + 1]\n",
        "                spaces.append(next_start > end)\n",
        "            else:\n",
        "                spaces.append(True)\n",
        "        return words, spaces\n",
        "\n",
        "    def __call__(self, text):\n",
        "        # TODO: Encode the texts to obtain tokens\n",
        "        tokens = None\n",
        "        # TODO: Use get_words_spaces to obtain the words and spaces\n",
        "        words, spaces = None, None\n",
        "        return Doc(self.vocab, words=words, spaces=spaces)\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "nlp.vocab = Vocab(strings=[\n",
        "    tok for tok in tokenizer.get_vocab().keys()\n",
        "])\n",
        "nlp.tokenizer = BPETokenizer(tokenizer, nlp.vocab)\n",
        "\n",
        "text = \"Jeff Bezos is a billionaire who became famous after the Dutch bridge controversy.\"\n",
        "# TODO: Convert the text in a list of tokens and print them"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Tokenization in different languages"
      ],
      "metadata": {
        "id": "8amBXhXgJZhR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most tokenizers we have seen so far are trained on English data. They often work reasonably well for the English language, but what about other languages? A major issue in multilingual Large LMs is that their shared subword vocabulary favors high-resource languages at the cost of the low-resource ones."
      ],
      "metadata": {
        "id": "R6FgA7b3J4Qt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand this problem, you can run different subword tokenizers on translations of the same sentence in different languages.\n",
        "\n",
        "Below, the first tokenizer belongs to the *multilingual* BLOOM model ([BLOOM](https://huggingface.co/bigscience/bloom)) which was trained on a mix of more than 40 languages. While model developers strived to balance the amount of different languages in the dataset, the distribution remains strongly uneven with most data being in English, followed by Chinese and French ([https://huggingface.co/bigscience/bloom#training-data]). This imbalance is even stronger in newer, larger LMs.\n",
        "\n",
        "The second tokenizer instead was trained on a 100mb-sized *monolingual* corpus ([GoldfishLM-NLD](https://huggingface.co/goldfish-models/nld_latn_100mb)). This is a much smaller corpus than the previous one, but contains only text in one language."
      ],
      "metadata": {
        "id": "JxTevTdhMxYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the three different types of tokenizers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Load the multilingual (BLOOM) tokenizer\n",
        "tokenizer_bloom = AutoTokenizer.from_pretrained(\"bigscience/bloom\")\n",
        "\n",
        "# Load monolingual tokenizers for several languages\n",
        "# (Here English and Dutch but many more Goldifh models are available!)\n",
        "tokenizer_eng = AutoTokenizer.from_pretrained(\"goldfish-models/eng_latn_100mb\")\n",
        "tokenizer_nld = AutoTokenizer.from_pretrained(\"goldfish-models/nld_latn_100mb\")\n"
      ],
      "metadata": {
        "id": "7utPbqPeltZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here are 4 sentences taken from Flores-200 dataset (https://github.com/facebookresearch/flores).\n",
        "# These dataset contains translations of the same sentences in 200 languages\n",
        "\n",
        "sentences_eng = [\n",
        "    'The pilot was identified as Squadron Leader Dilokrit Pattavee.',\n",
        "    'Local media reports an airport fire vehicle rolled over while responding.',\n",
        "    '28-year-old Vidal had joined Barça three seasons ago, from Sevilla.',\n",
        "    'Since moving to the Catalan-capital, Vidal had played 49 games for the club.'\n",
        "]\n",
        "\n",
        "sentences_nld = [\n",
        "    'De piloot werd geïdentificeerd als majoor Dilokrit Pattavee.',\n",
        "    'De lokale media meldt dat er tijdens een actie op de luchthaven een brandweerwagen is gekanteld.',\n",
        "    'De 28-jaar oude Vidal is drie seizoenen geleden van Sevilla naar Barça overgestapt.',\n",
        "    'Sinds hij verhuisde naar de Catalaanse hoofdstad heeft hij 49 wedstrijden gespeeld voor de club.'\n",
        "    ]"
      ],
      "metadata": {
        "id": "ZqK4tNd_mAsL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentences,tokenizer_mono in zip([sentences_eng, sentences_nld],\n",
        "                                    [tokenizer_eng, tokenizer_nld]):\n",
        "  for sent in sentences:\n",
        "    # print the token segmentations of each tokenizer\n",
        "    subwords_bloom = tokenizer_bloom.tokenize(sent)\n",
        "    subwords_mono  = tokenizer_mono.tokenize(sent)\n",
        "\n",
        "    # for better visualization:\n",
        "    subwords_bloom = [s.replace('Ġ','▁') for s in subwords_bloom]\n",
        "\n",
        "    print(' '.join(subwords_bloom))\n",
        "    print(' '.join(subwords_mono))\n",
        "\n",
        "    # calculate the length and see which one is the shortest/longest\n",
        "    # TODO: include the length results in your analysis\n",
        "    print(len(subwords_bloom), len(subwords_mono))\n",
        "    print()"
      ],
      "metadata": {
        "id": "E7L-zLmmmHZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXERCISE:\n",
        "\n",
        "Consider the 40+ languages supported by BLOOM ([https://huggingface.co/bigscience/bloom#training-data]).\n",
        "\n",
        "(a) What do you expect to determine the tokenizer's behavior on different languages? Is training data size the only explanation? What other factors may determine the tokenizer behavior?\n",
        "\n",
        "[*Provide a textual answer for (a) in a paragraph of 4-5 sentences.*]\n",
        "\n",
        "(b) Based on your reflection, choose 2 examples of languages on which you expect the multilingual tokenizer to do a decent job, and 2 others on which you expect it to work very poorly (i.e. to segment the text very aggressively)\n",
        "\n",
        "For each of the 4 languages, provide the subword counts on the FLORES benchmark by the multilingual ([BLOOM](https://huggingface.co/bigscience/bloom)) tokenizer versus that of the monolingual ([Goldfish](https://huggingface.co/collections/goldfish-models/100mb-goldfish-66c3c17d7be2e67389bfa67f)) tokenizer.\n",
        "\n",
        "Note: In a given language, a large difference between the multilingual tokenizer’s subword count and that of the corresponding monolingual tokenizer can be taken as a proxy of the underperformance of the multilingual tokenizer.\n",
        "\n",
        "[*Provide the counts in an easy-to-read format, by adding a short text to explain how you chose the languages.*]\n"
      ],
      "metadata": {
        "id": "yZo8O1Pvmdih"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkrqDqVLEyQq"
      },
      "source": [
        "## [Optional] Exercise 3: Lexicon-based Transduction System\n",
        "\n",
        "In this exercise you will build a rule-based tool to transduce a given input text **from masculine to feminine**. You are provided with a list of pairs including feminine words and their masculine counterparts. To create a rule based transducer, the following components will be needed:\n",
        "\n",
        "1. Extract a subset of sentences from the `wikitext-103-raw-v1` containing masculine words (words from the list, gendered pronouns (e.g. he/his/him)). **Tip**: you can try to use the spaCy lemmas annotations to avoid removing inflected forms of words.\n",
        "\n",
        "Fill the `is_masculine` function so that only sentences containing masculine words are preserved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-sR5071EyQq"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import datasets\n",
        "\n",
        "gender_lexicon = [\n",
        "    (\"Brother\", \"Sister\"),\n",
        "    (\"Drake\", \"Duck\"),\n",
        "    (\"Father\", \"Mother\"),\n",
        "    (\"Gentleman\", \"Lady\"),\n",
        "    (\"Husband\", \"Wife\"),\n",
        "    (\"Man\", \"Woman\"),\n",
        "    (\"Nephew\", \"Niece\"),\n",
        "    (\"Son\", \"Daughter\"),\n",
        "    (\"Wizard\", \"Witch\"),\n",
        "    (\"Boy\", \"Girl\"),\n",
        "    (\"Bull\", \"Cow\"),\n",
        "    (\"Cock\", \"Hen\"),\n",
        "    (\"Dog\", \"Bitch\"),\n",
        "    (\"Drone\", \"Bee\"),\n",
        "    (\"Gander\", \"Goose\"),\n",
        "    (\"Horse\", \"Mare\"),\n",
        "    (\"King\", \"Queen\"),\n",
        "    (\"Monk\", \"Nun\"),\n",
        "    (\"Sir\", \"Madam\"),\n",
        "    (\"Stag\", \"Hind\"),\n",
        "    (\"Stallion\", \"Mare\"),\n",
        "    (\"Tutor\", \"Governess\"),\n",
        "    (\"Drone\", \"Bee\"),\n",
        "    (\"Brother-in-law\", \"Sister-in-law\"),\n",
        "    (\"Son-in-law\", \"Daughter-in-law\"),\n",
        "    (\"Maternal-uncle\", \"Maternal-aunt\"),\n",
        "    (\"Step-son\", \"Step-daughter\"),\n",
        "    (\"Hostess\", \"Steward\"),\n",
        "    (\"Widow\", \"Widower\"),\n",
        "    (\"author\", \"authoress\"),\n",
        "    (\"count\", \"countess\"),\n",
        "    (\"heir\", \"heiress\"),\n",
        "    (\"manager\", \"manageress\"),\n",
        "    (\"patron\", \"patroness\"),\n",
        "    (\"priest\", \"priestess\"),\n",
        "    (\"baron\", \"baroness\"),\n",
        "    (\"giant\", \"giantess\"),\n",
        "    (\"host\", \"hostess\"),\n",
        "    (\"lion\", \"lioness\"),\n",
        "    (\"mayor\", \"mayoress\"),\n",
        "    (\"poet\", \"poetess\"),\n",
        "    (\"shepherd\", \"shepherdess\"),\n",
        "    (\"actor\", \"actress\"),\n",
        "    (\"conductor\", \"conductress\"),\n",
        "    (\"hunter\", \"huntress\"),\n",
        "    (\"prince\", \"princess\"),\n",
        "    (\"traitor\", \"traitress\"),\n",
        "    (\"master\", \"mistress\"),\n",
        "    (\"benefactor\", \"benefactress\"),\n",
        "    (\"founder\", \"foundress\"),\n",
        "    (\"instructor\", \"instructress\"),\n",
        "    (\"emperor\", \"empress\"),\n",
        "    (\"tiger\", \"tigress\"),\n",
        "    (\"waiter\", \"waitress\"),\n",
        "    (\"murderer\", \"murderess\"),\n",
        "    (\"hero\", \"heroine\"),\n",
        "    (\"fox\", \"vixen\"),\n",
        "    (\"sultan\", \"sultana\"),\n",
        "    (\"grandfather\", \"grandmother\"),\n",
        "    (\"manservant\", \"maidservant\"),\n",
        "    (\"milkman\", \"milkwoman\"),\n",
        "    (\"salesman\", \"saleswoman\"),\n",
        "    (\"great-uncle\", \"great-aunt\"),\n",
        "    (\"landlord\", \"landlady\"),\n",
        "    (\"he\", \"she\"),\n",
        "    (\"him\", \"her\"),\n",
        "    (\"his\", \"her\")\n",
        "]\n",
        "\n",
        "def is_masculine(text):\n",
        "    # TODO: Fill your regex with words from the wordlist\n",
        "    # (use '|'.join(...) to join them in the regex)\n",
        "    regex = None\n",
        "    return bool(re.search(regex, text, re.IGNORECASE))\n",
        "\n",
        "\n",
        "dataset = datasets.load_dataset(\n",
        "    \"wikitext\", \"wikitext-103-raw-v1\", split=\"train+test+validation\"\n",
        ")\n",
        "\n",
        "# We consider only the first 200 characters to avoid long paragraphs\n",
        "filtered_dataset = dataset.filter(lambda x: is_masculine(x[\"text\"][:200]))\n",
        "filtered_dataset = filtered_dataset.map(lambda x: {\"text\": x[\"text\"][:200]})\n",
        "filtered_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGRyBnV-EyQq"
      },
      "source": [
        "2. Create a `feminize` function that takes a sentence from the the filtered dataset and returns a feminized version of it, based on lexical pairs. Use it to create a new field \"feminine_text\" in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_Ew01KAEyQq"
      },
      "outputs": [],
      "source": [
        "def feminize(text):\n",
        "    \"\"\"Returns a feminized version of text\"\"\"\n",
        "    feminized_text = text\n",
        "    for m, f in gender_lexicon:\n",
        "        # TODO: fill in your regex to select word m (adapted from is_masculine)\n",
        "        match_regex = None\n",
        "        # TODO: fill in your regex to replace m by f\n",
        "        substitute_regex = None\n",
        "        feminized_text = re.sub(match_regex, substitute_regex, feminized_text, re.IGNORECASE)\n",
        "    return feminized_text\n",
        "\n",
        "# TODO: Use filtered_dataset.map to add a feminized version of the text column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwZJELd2EyQq"
      },
      "source": [
        "3. Rename the `text` field to `source_text` and the `feminine_text` field to `target_text` (this is needed for `SimpleT5` to work properly). Transform the dataset to Pandas DataFrame format and use the following code to train a simple neural transduction model.\n",
        "\n",
        "*(More info on the [T5 model](https://huggingface.co/t5-small) and the [SimpleT5](https://github.com/Shivanandroy/simpleT5) library)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BosmwjD0EyQq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from simplet5 import SimpleT5\n",
        "\n",
        "# TODO: Convert the Huggingface Dataset in a Pandas dataframe and split it in training\n",
        "# and evaluation sets (you decide the sizes based on your computational resources)\n",
        "train_df, eval_df = None, None\n",
        "\n",
        "model = SimpleT5()\n",
        "model.from_pretrained(model_type=\"t5\", model_name=\"t5-small\")\n",
        "model.train(\n",
        "    train_df=train_df,\n",
        "    eval_df=eval_df,\n",
        "    source_max_token_len=128,\n",
        "    target_max_token_len=128,\n",
        "    batch_size=8, max_epochs=3, use_gpu=torch.cuda.is_available()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEfo8F00EyQq"
      },
      "source": [
        "4. Conclude by testing the model on a few examples of your choice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lYHefTFEyQq"
      },
      "outputs": [],
      "source": [
        "model.load_model(\"t5\", \"<YOUR SAVED MODEL PATH>\", use_gpu=torch.cuda.is_available())\n",
        "\n",
        "text_to_feminize = \"my brother thought that his uncle was a duke\"\n",
        "model.predict(text_to_feminize)"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "f9f85d98b63f393548f3009c8d52d8286e609a1467b1184fe464fb700873fbd3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
