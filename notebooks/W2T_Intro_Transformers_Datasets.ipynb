{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gsarti/ik-nlp-tutorials/blob/main/notebooks/W2T_Intro_Transformers_Datasets.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run in Colab to install local packages\n",
    "!pip install transformers torch datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to 🤗 Transformers and 🤗 Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This tutorial is based off some chapters of the [HuggingFace Course](https://huggingface.co/course/chapter1/1), take a look for a more detailed overview!*\n",
    "\n",
    "Transformer models are nowadays the state-of-the-art and de-facto standard to solve all kinds of NLP tasks, from tagging to machine translation, to text classification.\n",
    "\n",
    "The usage of these models has been widely simplified and democratized by [HuggingFace](https://huggingface.co/) (🤗 in short), the startup behind the popular [🤗 Transformers library](https://huggingface.co/transformers/). The 🤗 Transformers library is completely open-source and provides a unified framework to create, train and use many transformer-based models, accompanied by a Cloud-hosting service called [Model hub](https://huggingface.co/models) (similar to Pytorch and Tensorflow Hubs) in which every user can host and share pre-trained and fine-tuned open-source models, and which currently contains over 25k models (as of Jan 24, 2022).\n",
    "\n",
    "We are going to start with a quick overview of the 🤗 Transformers library and its usage, and then we will dive into the 🤗 Datasets library, which is the largest open collection of text datasets ready for usage with 🤗 Transformers and other machine learning frameworks (also hosted on the [Dataset hub](https://huggingface.co/datasets)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "The most basic object in the 🤗 Transformers library is the pipeline() function. It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9915691018104553}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\"I've been waiting for the IK-NLP course for my whole life.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple sentences can also be passed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9807901382446289},\n",
       " {'label': 'NEGATIVE', 'score': 0.9996439218521118}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\n",
    "    [\"I've been waiting for this course for my whole life.\", \"I hate this course so much!\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, this pipeline selects a particular pretrained model that has been fine-tuned for sentiment analysis in English (`distilbert-base-uncased-finetuned-sst-2-english`). The model is downloaded and cached when you create the classifier object. If you rerun the command, the cached model will be used instead and there is no need to download the model again.\n",
    "\n",
    "There are three main steps involved when you pass some text to a pipeline:\n",
    "\n",
    "- The text is preprocessed into a format the model can understand.\n",
    "- The preprocessed inputs are passed to the model.\n",
    "- The predictions of the model are post-processed, so you can make sense of them.\n",
    "\n",
    "Some of the [currently available pipelines](https://huggingface.co/transformers/main_classes/pipelines.html) are:\n",
    "\n",
    "- `sentiment-analysis`\n",
    "- `text-generation`\n",
    "- `fill-mask` (filling a masked token or span with a predicted one)\n",
    "- `text2text-generation`\n",
    "- `ner` (named entity recognition)\n",
    "- `question-answering`\n",
    "\n",
    "Some pipelines, such as `sentiment-analysis`, `translation` and `summarization` are abstractions over other, more general ones (e.g. `summarization` and `translation` are abstractions over `text2text-generation`, `sentiment-analysis` over `text-classification`).\n",
    "\n",
    "Let's see some examples of pipelines in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text generation \n",
    "\n",
    "The text generation (i.e. autoregressive language modeling) setting has become widely popularized by models such as [GPT-3](https://en.wikipedia.org/wiki/GPT-3). Given a prompt, the model will auto-complete it by generating the remaining text. This is similar to the predictive text feature that is found on many phones. Here is an example using a small [GPT-2](https://huggingface.co/gpt2) model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 (https://huggingface.co/gpt2)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The goal of the course is to ensure that students are familiar with a number of fundamental techniques and algorithms in the area of natural language processing, such as: vernacular, declarative and post hoc, syntactic forms and abstractions, and'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\")\n",
    "generator(\n",
    "    \"The goal of the course is to ensure that students are familiar with a number of fundamental \"\n",
    "    \"techniques and algorithms in the area of natural language processing, such as: \"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">🤡 **Fun Fact**: Language models trained to perform autoregressive language modeling are already widely used in the industry. For example, the [Github Copilot](https://copilot.github.com/) integrated to the VisualStudio Code editor is a model trained to autocomplete text and code snippets, and is currently helping me in writing these notebooks (grey text are model suggestions).\n",
    "\n",
    "![Github Copilot autocompletion](../img/copilot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try controlling how many different sequences are generated with the argument `num_return_sequences` and the total maximal length of the output text with the argument `max_length`.\n",
    "\n",
    "We can use any custom model from the [Model hub](https://huggingface.co/models) by simply passing it (or its identifier) when creating the pipeline. We'll now use a [Dutch GPT-2 model](https://huggingface.co/GroNLP/gpt2-small-dutch) pretrained by our colleagues to generate some Dutch text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Het doel van de cursus is ervoor te zorgen dat studenten bekend zijn met een aantal fundamentele technieken en algoritmen op het gebied van natuurlijke taalverwerking, zoals:\\nDifferentiële differentiaalvergelijkingen voor grammatica. De term wordt gebruikt om wiskundige bewerkingen die mogelijk worden uitgevoerd in complexe systemen uit andere talen (bijvoorbeeld Engels of Frans). In deze benadering kan men kiezen tussen drie verschillende soorten contextudes - twee variëteitsmatrixconformaties ('decoratie' genoemd)\"}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"GroNLP/gpt2-small-dutch\")\n",
    "generator(\n",
    "    \"Het doel van de cursus is ervoor te zorgen dat studenten bekend zijn met een aantal \"\n",
    "    \"fundamentele technieken en algoritmen op het gebied van natuurlijke taalverwerking, zoals:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can refine your search for a model by clicking on the language tags, and pick a model that will generate text in another language. The Model Hub even contains checkpoints for multilingual models that support several languages.\n",
    "\n",
    "Once you select a model by clicking on it, you’ll see that there is a widget enabling you to try it directly online. This way you can quickly test the model’s capabilities before downloading it. More info on text generation here: [https://huggingface.co/tasks/text-generation](https://huggingface.co/tasks/text-generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask-filling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fill-mask` (i.e. masked language modeling) pipeline is used to fill masked tokens with a predicted token, which is a common pre-training task for encoder-only transformers leveraging bidirectional context such as [BERT](https://huggingface.co/bert-base-uncased). This is useful to fill gaps in the text with the most likely answer given the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base (https://huggingface.co/distilroberta-base)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'This course will teach you all about natural language processing models.',\n",
       "  'score': 0.5701010823249817,\n",
       "  'token': 1632,\n",
       "  'token_str': ' natural'},\n",
       " {'sequence': 'This course will teach you all about functional language processing models.',\n",
       "  'score': 0.06381034106016159,\n",
       "  'token': 12628,\n",
       "  'token_str': ' functional'},\n",
       " {'sequence': 'This course will teach you all about programming language processing models.',\n",
       "  'score': 0.043610505759716034,\n",
       "  'token': 8326,\n",
       "  'token_str': ' programming'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\")\n",
    "unmasker(\"This course will teach you all about <mask> language processing models.\", top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `top_k` argument controls how many possibilities you want to be displayed. Note that here the model fills in the special `<mask>` word, which is often referred to as a mask token. Other mask-filling models might have different mask tokens, so it’s always good to verify the proper mask word when exploring other models. One way to check it is by looking at the mask word used in the widget. More info on mask-filling here: [https://huggingface.co/tasks/fill-mask](https://huggingface.co/tasks/fill-mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text2Text Generation\n",
    "\n",
    "The `text2text-generation` pipeline encompasses all the sequence-to-sequence tasks on which a model was trained. We're gonna cover Encoder-Decoder architectures in week 6, but in the meantime you can see here some examples of sequence-to-sequence tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': ' America has changed dramatically during recent years . The number of engineering graduates in the U.S. has declined in traditional engineering disciplines such as mechanical, civil,    electrical, chemical, and aeronautical engineering . Rapidly developing economies such as China and India continue to encourage and advance the teaching of engineering .'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summarization: reduce a text to its summary\n",
    "# The same result can be achieved using the `summarization` pipeline.\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"text2text-generation\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
    "summarizer(\n",
    "    \"\"\"\n",
    "    America has changed dramatically during recent years. Not only has the number of \n",
    "    graduates in traditional engineering disciplines such as mechanical, civil, \n",
    "    electrical, chemical, and aeronautical engineering declined, but in most of \n",
    "    the premier American universities engineering curricula now concentrate on \n",
    "    and encourage largely the study of engineering science. As a result, there \n",
    "    are declining offerings in engineering subjects dealing with infrastructure, \n",
    "    the environment, and related issues, and greater concentration on high \n",
    "    technology subjects, largely supporting increasingly complex scientific \n",
    "    developments. While the latter is important, it should not be at the expense \n",
    "    of more traditional engineering.\n",
    "\n",
    "    Rapidly developing economies such as China and India, as well as other \n",
    "    industrial countries in Europe and Asia, continue to encourage and advance \n",
    "    the teaching of engineering. Both China and India, respectively, graduate \n",
    "    six and eight times as many traditional engineers as does the United States. \n",
    "    Other industrial countries at minimum maintain their output, while America \n",
    "    suffers an increasingly serious decline in the number of engineering graduates \n",
    "    and a lack of well-educated engineers.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'This laboratory was adapted from the original course by HuggingFace.'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Translation: Translate a sentence from French to English\n",
    "# The same result can be achieved using the `translation` pipeline.\n",
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\"text2text-generation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "translator(\"Ce laboratoire a été adapté à partir du cours originel par HuggingFace.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes our pipeline overview. Refer to the [documentation](https://huggingface.co/docs/transformers/v4.15.0/en/main_classes/pipelines) for additional information on pipeline types and parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behind the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in the previous chapter, a `pipeline` has a preprocessing step, a model inference step and a post-processing step:\n",
    "\n",
    "<div>\n",
    "<img src=\"https://huggingface.co/course/static/chapter2/full_nlp_pipeline.png\", alt=\"Visual representation of a full NLP pipeline\" width=\"80%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like other neural networks, Transformer models can’t process raw text directly, so the first step of our pipeline is to convert the text inputs into numbers that the model can make sense of. To do this we use a tokenizer, which will be responsible for:\n",
    "\n",
    "- Splitting the input into words, subwords, or symbols (like punctuation) that are called tokens\n",
    "- Mapping each token to an integer\n",
    "- Adding additional inputs that may be useful to the model\n",
    "\n",
    "All this preprocessing needs to be done in exactly the same way as when the model was pretrained, so we first need to download that information from the Model Hub. To do this, we use the `AutoTokenizer` class and its `from_pretrained` method. Using the checkpoint name of our model, it will automatically fetch the data associated with the model’s tokenizer and cache it (so it’s only downloaded the first time you run the code below).\n",
    "\n",
    "**Important:** Every type of transformer model has its own tokenizer and model classes (e.g. `BertTokenizer`, `GPT2Tokenizer`, `XLMTokenizer`, ...). The `AutoTokenizer` and `AutoModel` classess will rely on configurations saved alongside model checkpoints to load any model in the right class, so you don’t need to worry about this.\n",
    "\n",
    "Let's try to load and use the tokenizer of the sentiment analysis model we tried at the beginning of this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 30522 tokens\n",
      "10 random tokens: {'ٹ': 1301, 'vegetarian': 23566, 'notice': 5060, 'camps': 7958, 'regret': 9038, 'ngo': 17895, '##₃': 11622, 'retaining': 12823, 'candace': 22905, 'kicking': 10209}\n",
      "==================== \n",
      " Output: {'input_ids': tensor([[ 101, 1045, 1005, 2310, 2042, 3403, 2005, 2023, 2607, 2005, 2026, 2878,\n",
      "         2166, 1012,  102],\n",
      "        [ 101, 1045, 5223, 2023, 2607, 2061, 2172,  999,  102,    0,    0,    0,\n",
      "            0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "identifier = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(identifier)\n",
    "\n",
    "print(\"Vocabulary size:\", len(tokenizer), \"tokens\")\n",
    "print(\"10 random tokens:\", {k:v for i, (k,v) in enumerate(tokenizer.vocab.items()) if i < 10})\n",
    "\n",
    "raw_inputs = [\"I've been waiting for this course for my whole life.\", \"I hate this course so much!\"]\n",
    "inputs = tokenizer(raw_inputs, padding=True, return_tensors=\"pt\")\n",
    "print(\"=\"*20,\"\\n\",\"Output:\",inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `padding` argument tells the tokenizer to make sure all input sequences are encoded with the same length. This is important since the model needs batches of IDs having the same size to work properly. The `return_tensors` argument tells the tokenizer to return the output as a PyTorch tensor instead of a list of tokens. This is important since the model expects the input to be a tensor.\n",
    "\n",
    "The output itself is a dictionary containing two keys, `input_ids` and `attention_mask`. `input_ids` contains two rows of integers (one for each sentence) that are the unique identifiers of the tokens in each sentence. The `attention_mask` is used by the model to ignore padding tokens when computing the loss.\n",
    "\n",
    "We can recover the original tokens from input ids as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'i', \"'\", 've', 'been', 'waiting', 'for', 'this', 'course', 'for', 'my', 'whole', 'life', '.', '[SEP]']\n",
      "['[CLS]', 'i', 'hate', 'this', 'course', 'so', 'much', '!', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]))\n",
    "print(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The special `[CLS]` and `[SEP]` tokens are used by BERT-like models to delimit the sentences, and they are automatically added by the tokenizer.\n",
    "\n",
    "We can now proceed to download the model and perform inference over the input ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertModel: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "identifier = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(identifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This architecture contains only the base Transformer module: given some inputs, it outputs what we’ll call **hidden states**, also known as features. For each model input, we’ll retrieve a high-dimensional vector representing the contextual representation of that token in the model's learned embedding space.\n",
    "\n",
    "While these hidden states can be useful on their own, they’re usually inputs to another part of the model, known as the **head**, which is responsible to perform the actual prediction associated to the target task.\n",
    "\n",
    "The input vector to the model is usually three-dimensional, containing respectively\n",
    "\n",
    "- Batch size: The number of sequences processed at a time (2 in our example).\n",
    "- Sequence length: The length of the numerical representation of the sequence (19 in our example).\n",
    "- Hidden size: The vector dimension of each model input. The vector is said to be “high dimensional” because of this last value. The hidden size can be very large (768 is common for smaller models, and in larger models this can reach 3072 or more).\n",
    "\n",
    "We can now feed the `inputs` produced by the tokenizer to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 15, 768])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the same values that could be extracted by the `feature-extraction` pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model heads take the high-dimensional vector of hidden states as input and project them onto a different dimension. They are usually composed of one or a few linear layers:\n",
    "\n",
    "<div>\n",
    "<img src=\"https://huggingface.co/course/static/chapter2/transformer_and_head.png\", alt=\"A Transformer model\" width=\"80%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the Transformer model is sent directly to the model head to be processed.\n",
    "\n",
    "In this diagram, the model is represented by its embeddings layer and the subsequent layers. The embeddings layer converts each input ID in the tokenized input into a vector that represents the associated token. The subsequent layers manipulate those vectors using the attention mechanism to produce the final representation of the sentences.\n",
    "\n",
    "There are many different architectures available in 🤗 Transformers, with each one designed around tackling a specific task. Here is a non-exhaustive list:\n",
    "\n",
    "- *Model (default headless model)\n",
    "- *ForCausalLM\n",
    "- *ForMaskedLM\n",
    "- *ForMultipleChoice\n",
    "- *ForQuestionAnswering\n",
    "- *ForSequenceClassification\n",
    "- *ForTokenClassification\n",
    "- *ForConditionalGeneration\n",
    "\n",
    "For our example, we will need a model with a sequence classification head (to be able to classify the sentences as positive or negative). So, we won’t actually use the `AutoModel` class, but `AutoModelForSequenceClassification`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we look at the shape of our inputs, the dimensionality will be much lower: the model head takes as input the high-dimensional vectors we saw before, and outputs vectors containing two values (one per label). Since we have just two sentences and two labels, the result we get from our model is of shape 2 x 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocessing outputs\n",
    "\n",
    "The values we get as output from our model don’t necessarily make sense by themselves. Let’s take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.0023,  1.9307],\n",
      "        [ 4.4057, -3.5342]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model predicted [-1.5607, 1.6123] for the first sentence and [ 4.1692, -3.3464] for the second one. Those are not probabilities but logits, the raw, unnormalized scores outputted by the last layer of the model. To be converted to probabilities, they need to go through a SoftMax layer (all 🤗 Transformers models output the logits, as the loss function for training will generally fuse the last activation function, such as SoftMax, with the actual loss function, such as cross entropy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.9210e-02, 9.8079e-01],\n",
      "        [9.9964e-01, 3.5611e-04]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that the model predicted [0.0192, 0.9807] for the first sentence and [0.9996, 0.0004] for the second one. These are recognizable probability scores.\n",
    "\n",
    "To get the labels corresponding to each position, we can inspect the `id2label` attribute of the model configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can conclude that the model predicted the following:\n",
    "\n",
    "- First sentence: NEGATIVE: 0.0192, POSITIVE: 0.9807\n",
    "- Second sentence: NEGATIVE: 0.9996, POSITIVE: 0.0004\n",
    "\n",
    "We have successfully reproduced the three steps of the pipeline: preprocessing with tokenizers, passing the inputs through the model, and postprocessing. Here is a summary of all the steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've been waiting for this course for my whole life. {'NEGATIVE': 0.019209884107112885, 'POSITIVE': 0.9807901382446289}\n",
      "I hate this course so much! {'NEGATIVE': 0.9996439218521118, 'POSITIVE': 0.0003561103658284992}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "identifier = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(identifier)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(identifier)\n",
    "sequences = [\"I've been waiting for this course for my whole life.\", \"I hate this course so much!\"]\n",
    "\n",
    "\n",
    "tokens = tokenizer(sequences, padding=True, return_tensors=\"pt\")\n",
    "output = model(**tokens)\n",
    "probs = torch.nn.functional.softmax(output.logits, dim=-1).tolist()\n",
    "for i, p in enumerate(probs):\n",
    "    print(sequences[i], {model.config.id2label[j]:v for j, v in enumerate(p)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, the `AutoModel` class and all of its relatives are actually simple wrappers over the wide variety of models available in the library. It’s a clever wrapper as it can automatically guess the appropriate model architecture for your checkpoint, and then instantiates a model with this architecture. However, if you know the type of model you want to use, you can use the class that defines its architecture directly. Let’s take a look at how this works with a BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# Building the config\n",
    "config = BertConfig()\n",
    "\n",
    "# Building the model from the config\n",
    "model = BertModel(config)\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration contains many attributes that are used to build the model. While you haven’t seen what all of these attributes do yet, you should recognize some of them: the `hidden_size` attribute defines the size of the `hidden_states` vector, and `num_hidden_layers` defines the number of layers the Transformer model has.\n",
    "\n",
    "Creating a model from the default configuration initializes it with random values. The model can be used in this state, but it will output gibberish; it needs to be trained first. However, this procedure requires a long time and a lot of data. To avoid unnecessary and duplicated effort, it’s imperative to be able to share and reuse models that have already been trained.\n",
    "\n",
    "Loading a Transformer model that is already trained is simple — we can do this using the `from_pretrained` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is now initialized with all the weights of the checkpoint. It can be used directly for inference on the tasks it was trained on, and it can also be fine-tuned on a new task. By training with pretrained weights rather than from scratch, we can quickly achieve good results.\n",
    "\n",
    "The weights have been downloaded and cached (so future calls to the from_pretrained() method won’t re-download them) in the cache folder, which defaults to ~/.cache/huggingface/transformers. You can customize your cache folder by setting the HF_HOME environment variable.\n",
    "\n",
    "The identifier used to load the model can be the identifier of any model on the Model Hub, as long as it is compatible with the BERT architecture.\n",
    "\n",
    "Saving a model is as easy as loading one — we use the `save_pretrained`() method, which is analogous to the `from_pretrained`() method. This saves two files to your disk: the configuration file and the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"directory_on_my_computer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same can be accomplished for a tokenizer. This saves the essential files to restore the tokenizer object using `from_pretrained(directory_on_my_computer)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokenizer.save_pretrained(\"directory_on_my_computer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes our quick overview of the `transformers` library. You can refer to the extended version of this introduction at the original [HuggingFace Course](https://huggingface.co/course) for more examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the 🤗 Datasets library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🤗 Datasets provides loading scripts to handle the loading of local and remote datasets. It supports several common data formats, such as:\n",
    "\n",
    "Data format \t   |Loading script |Example\n",
    "-------------------|---------------|----------------------------------------------|\n",
    "CSV & TSV \t       |csv \t       |`load_dataset(\"csv\", data_files=\"my_file.csv\")`|\n",
    "Text files \t       |text \t       |`load_dataset(\"text\", data_files=\"my_file.txt\")`|\n",
    "JSON & JSON Lines  |json \t       |`load_dataset(\"json\", data_files=\"my_file.jsonl\")`|\n",
    "Pickled DataFrames |pandas \t       |`load_dataset(\"pandas\", data_files=\"my_dataframe.pkl\")`|\n",
    "\n",
    "As shown in the table, for each data format we just need to specify the type of loading script in the load_dataset() function, along with a data_files argument that specifies the path to one or more files. Let’s start by loading a dataset from local files; later we’ll see how to do the same with remote files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "def dot_score(a: Tensor, b: Tensor):\n",
    "    \"\"\"\n",
    "    Computes the dot-product dot_prod(a[i], b[j]) for all i and j.\n",
    "    :return: Matrix with res[i][j]  = dot_prod(a[i], b[j])\n",
    "    \"\"\"\n",
    "    if not isinstance(a, torch.Tensor):\n",
    "        a = torch.tensor(a)\n",
    "    if not isinstance(b, torch.Tensor):\n",
    "        b = torch.tensor(b)\n",
    "    if len(a.shape) == 1:\n",
    "        a = a.unsqueeze(0)\n",
    "    if len(b.shape) == 1:\n",
    "        b = b.unsqueeze(0)\n",
    "    print(a.shape, b.shape)\n",
    "    return torch.mm(a, b.transpose(0, 1))\n",
    "\n",
    "#Mean Pooling - Take average of all tokens\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Я не понимаю, что говорит ассистент учителя\"\n",
    "candidates = [\n",
    "    \"Primero ejemplo de una oracion en español\", \n",
    "    \"Totally unrelated text, to be ignored\",\n",
    "    \"I do not understand what the teaching assistant is saying\",\n",
    "    \"Ce texte n'a aucun rapport avec la phrase originelle\",\n",
    "    \"Non capisco cosa sta dicendo\"\n",
    "]\n",
    "\n",
    "q_feats = mean_pooling(extractor(query)[0]\n",
    "c_feats = extractor(candidates)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 768]) torch.Size([1, 11, 768])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat2 must be a matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-24974686f2e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdot_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_feats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-84a3d4a207a7>\u001b[0m in \u001b[0;36mdot_score\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: mat2 must be a matrix"
     ]
    }
   ],
   "source": [
    "dot_score(q_feats, c_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9f85d98b63f393548f3009c8d52d8286e609a1467b1184fe464fb700873fbd3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
